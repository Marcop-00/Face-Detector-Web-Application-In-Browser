\documentclass[12pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{float}

% Page Geometry
\geometry{
 a4paper,
 total={170mm,257mm},
 left=20mm,
 top=20mm,
}

% Code Listing Styles
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegray},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{red},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}

\title{\textbf{Project: B10 - AI-Assisted Face Detector Web Application} \\ \large Output Validation Report}

\author{Marco Pernisco (matr: 683674)\\
Master's degree in Computer Science - University of Pisa}

\begin{document}

\maketitle

\tableofcontents
\newpage

\section{Introduction}

The objective of this project was to design and implement a browser-based face detection application with a focus on privacy and client-side execution. Unlike traditional computer vision systems that rely on backend processing, this application leverages modern web technologies—specifically JavaScript and WebAssembly (WASM)—to perform Machine Learning (ML) inference directly within the user's browser.

The development methodology for this project utilized a Generative AI assistant to accelerate the prototyping phase. The primary goal was to achieve a functional "One-Shot" solution—a working prototype generated from a single, highly optimized prompt, minimizing the need for iterative debugging. This report details the planning, prompt engineering strategy, security considerations, and deployment pipeline for the application.

\section{Planning}

The planning phase focused less on writing code manually and more on defining the architectural constraints for the AI assistant. The core requirements identified were:
\begin{itemize}
    \item \textbf{Client-Side Execution:} No video data could be sent to a server.
    \item \textbf{Real-Time Performance:} The app must handle live video streams with minimal latency.
    \item \textbf{Visual Feedback:} Bounding boxes must be overlaid on detected faces.
\end{itemize}

\subsection{Prompt Optimization}

To maximize the probability of a successful "One-Shot" generation, a "Meta-Prompting" strategy was employed. Instead of immediately asking for the code, the AI was first tasked with refining the request itself.
The initial raw request was: \textit{"I have to do this project, could you generate an optimized prompt for an LLM like you? I want to build it in one shot."}
This step was crucial because LLMs perform better when the prompt explicitly outlines constraints, libraries, and expected outputs. The AI generated a detailed technical prompt that specified the use of browser-based ML libraries and defined the testing criteria, ensuring the subsequent code generation would be contextually accurate.
This iterative meta-prompting strategy is supported by recent research on \textbf{Optimization by PROmpting (OPRO)}, which demonstrates that LLMs can effectively optimize their own instructions to improve performance more than the human-written prompts alone \cite{yang2023opro}.\\

\textbf{Link to Original Chat:} \url{https://gemini.google.com/share/2dc8f76950b5}

\subsection{One-Shot Solution}

Using the optimized prompt, the AI generated a complete, single-file solution stack utilizing Google's MediaPipe Tasks Vision API. The generated application adhered strictly to the "client-side only" requirement by loading the BlazeFace (Short Range) model via WebAssembly.

The solution architecture consisted of four distinct modules:
\begin{enumerate}
    \item \textbf{Initialization Phase:} The application asynchronously loads the \texttt{FilesetResolver} and initializes the \texttt{FaceDetector} class. Notably, the AI configured the detector to use the \texttt{GPU} delegate, ensuring hardware acceleration for smooth performance.
    \item \textbf{Video Pipeline:} Access to the webcam is managed via\\
     \texttt{navigator.mediaDevices.getUserMedia}, with a fallback to a specific resolution (1280x720) ideal for the BlazeFace model.
    \item \textbf{Inference Loop:} A \texttt{requestAnimationFrame} loop creates a continuous stream. For every frame, the application calls \texttt{faceDetector.detectForVideo()}, calculates the inference latency, and updates a "Heads-Up Display" (HUD) overlay.
    \item \textbf{Visualization:} The results are rendered onto an HTML5 Canvas. The AI implemented detailed drawing logic, including bounding boxes and six facial keypoints (eyes, nose, mouth, ears), applying a mirror transformation to align naturally with the user's movement.
\end{enumerate}


\section{Security Improvements}

To address privacy concerns regarding biometric data, a specific consultation was conducted to ensure no video data leaves the client. The security strategy relies on a "Defense in Depth" approach.
The prompt used was: \textit{How can I ensure that any image is sent to a server? I would like to guarantee that the client doesn’t connect to any new remote source.}

My idea was to stress the AI to find a way to guarantee that no data exfiltration occurs.
The primary mechanism for preventing data exfiltration is a strict Content Security Policy (CSP). This functions as a whitelist, instructing the browser to block any network request to unauthorized domains.

\subsection{Refinements and Debugging}

During the integration of the CSP, an initial implementation error was encountered. The first iteration of the policy included inline comments within the HTML \texttt{content} attribute, causing parsing errors. Furthermore, it was determined that the WebAssembly runtime requires specific permissions to compile binaries.

The prompt used to fix the CSP was: \textit{copying and pasting the meta tag I'm receiving errors in console, I think that the comments inside are written in a wrong way.}

The policy was refined by removing comments and adding the \texttt{'unsafe-eval'} directive, which is required for JIT compilation in browser-based ML libraries.

\subsection{Verification Procedures}

To validate the "Local Only" claim, I decided to open a new conversation because I would to go in the details of how to verify that no data is sent to a server.\\
The prompt used was: \textit{Hi, I have a webapp for face detection that must work only client side without any type of connections with other servers.
I've added this in my html file to increase security, but is that secure? The tag unsafe tell me the opposite. (here I've inserted the meta tag)}.
The AI suggested me to use a wasm-unsafe-eval directive giving at least the permission to compile the wasm binaries, but no other connections are allowed.\\
But trying this CSP I was still receiving errors in the console about blocked connections to some external sources.\\
I thounght that the best way to ensure a good security was to download all the assets locally, including the WASM files and the model files, to ensure that no external connections are required at all.\\
The prompt used to confirm my idea was: \textit{Ok, I think that wasm can be a reasonable solution, otherwise the alternative 100\% secure solution, I think,  can be have everything installed in local, right?}
The AI confirmed my idea, suggesting to download the WASM files and the model files locally, and to adjust the code to load these assets from local paths instead of remote URLs.\\

\textbf{Link to Original Chat:} \url{https://gemini.google.com/share/5ffcb6b52e22}\\

Doing it manually would take a lot of time, so I decided to ask to the agent on VS Code, to modify the code to load the assets locally.\\
After the modifications I verified them and then proceeded to accept them.

\section{Deployment Strategy}

To ensure the application runs consistently across different environments I used a prompt asking for containerization options: \textit{I want to be sure that this web-app works in different environments without distinction.
How can I deploy it? Should I containerize it?}.\\
My Idea was to have a lightweight container that serves the static HTML file, because I don't want to get "on my machine it works" issues.
The AI suggested me to use three main options:
\begin{itemize}
    \item \textbf{Static Cloud:} Services like GitHub Pages or Netlify
    \item \textbf{Docker:} Using a lightweight web server like Nginx or Caddy
    \item \textbf{Simple Python/Node Server:} Using built-in HTTP servers for quick testing
\end{itemize}
I ended up choosing the Docker option, because it guarantees the same environment everywhere, but with a constraint: the HTTPS challenge, to ensure secure camera access.

\subsection{The HTTPS Challenge in Isolated Environments}
My prompt was: \textit{Ok, I want to use Docker and to configure also the the HTTPS}.\\
A critical constraint identified during the planning phase is the browser's strict security model regarding hardware access.
The API used for the camera, \texttt{navigator.mediaDevices.getUserMedia}, is restricted to **Secure Contexts**. 
\begin{itemize}
    \item \textbf{The Constraint:} Browsers strictly \textbf{block} camera access on insecure HTTP connections (e.g., \texttt{http://192.168.1.50}).
    \item \textbf{The Problem:} Standard Docker containers serve traffic over plain HTTP (port 80) by default.
    \item \textbf{The Solution:} The container must handle its own encryption.
\end{itemize}

\subsection{Implementation: Standalone Secure Container}

To create a truly portable solution that allows camera access immediately upon deployment (without requiring an external Reverse Proxy or a purchased domain), an automated "Self-Signed" strategy was implemented.

This approach involves two key components generated during the build process:

\subsubsection{1. Automated Certificate Generation (Dockerfile)}
Instead of manually mounting certificates, the \texttt{Dockerfile} was modified to install \texttt{OpenSSL} and programmatically generate a valid X.509 certificate during the image build. This ensures that every instance of the container has unique, valid cryptographic keys.

\begin{lstlisting}[language=bash, caption=Dockerfile with Automatic SSL Generation]
# Base image: Lightweight Nginx
FROM nginx:alpine

# 1. Install OpenSSL
RUN apk add --no-cache openssl

# 2. Create SSL directory
RUN mkdir -p /etc/nginx/ssl

# 3. Generate a Self-Signed Certificate (Valid 365 days)
RUN openssl req -x509 -nodes -days 365 \
    -newkey rsa:2048 \
    -keyout /etc/nginx/ssl/selfsigned.key \
    -out /etc/nginx/ssl/selfsigned.crt \
    -subj "/C=US/ST=State/L=City/O=Organization/CN=localhost"

# 4. Copy Configurations
COPY nginx.conf /etc/nginx/nginx.conf
COPY face_app.html /usr/share/nginx/html/index.html

# Expose HTTP and HTTPS ports
EXPOSE 80
EXPOSE 443

CMD ["nginx", "-g", "daemon off;"]
\end{lstlisting}

\subsubsection{2. Secure Server Configuration (Nginx)}
A custom \texttt{nginx.conf} was injected to enforce security. It performs two critical functions:
\begin{enumerate}
    \item \textbf{Force HTTPS:} Any request on Port 80 is strictly redirected to Port 443 (HTTPS).
    \item \textbf{SSL Termination:} It uses the generated certificates to encrypt the traffic before it leaves the container.
\end{enumerate}

\begin{lstlisting}[language=bash, caption=Nginx Configuration for SSL Termination]
server {
    listen 80;
    server_name localhost;
    return 301 https://$host$request_uri;
}

server {
    listen 443 ssl;
    server_name localhost;

    ssl_certificate /etc/nginx/ssl/selfsigned.crt;
    ssl_certificate_key /etc/nginx/ssl/selfsigned.key;
    
    # Modern TLS Protocols
    ssl_protocols TLSv1.2 TLSv1.3;
}
\end{lstlisting}

\subsection{Operational Verification}

Upon running the container (`docker run -p 443:443 ...`), the application is accessible via \texttt{https://localhost}.



\textbf{Note on User Experience:} Because the certificate is self-signed rather than issued by a public Certificate Authority (CA), browsers will display a "Not Secure" or "Your connection is not private" warning. 
However, for the purpose of this project, this is acceptable behavior. The connection \textbf{is} encrypted, and the \textbf{Secure Context} requirement is technically satisfied, allowing the \texttt{getUserMedia} API to unlock the camera and the face detection to function on remote mobile devices connected to the same network.\\
\textbf{Link to Original Chat:} \url{https://gemini.google.com/share/2723ab111b63}

\subsection{Additional features development}
Additional features were developed in a second phase after the initial "One-Shot" prototype using agent Gemini 3.0 Pro in the VS Code environment. The features implemented were:
\begin{itemize}
    \item \textbf{Blur Functionality:} A blur effect was implemented to obscure detected faces, enhancing user privacy.
    \item \textbf{Video Recording:} A recording feature was added, allowing users to capture video streams with face detection overlays. 
\end{itemize}

I would like to highlight that also the refinement and the division of the code in multiple files was done using the AI, to keep the code organized and maintainable, without spending time doing it manually and maybe making mistakes with all the paths.

\section {Development Platform and Tools}
The project was developed using VS Code integrated with the GitHub Copilot agent. This environment was chosen due to my existing workflow familiarity and the access provided by the GitHub Student Developer Pack. After evaluating various premium models available within Copilot, I selected Gemini 3.0 Pro, as it consistently demonstrated the highest level of capability for this specific use case.
I would like to highlight that the prompt optimization strategy were performed using Gemini 3.0 with Reasoning, while the code generation was done using Gemini 3.0 Pro, because more advanced and suggested for coding tasks.


\section{Cross-browser Testing}

To ensure reliability across diverse environments, a compatibility test was conducted on four major browser engines. The application was deployed via the Docker container and accessed over the local network.

\subsection{Test Results}

\begin{table}[h]
\centering
\scriptsize
\begin{tabular}{|l|l|l|}
\hline
\textbf{Browser} & \textbf{Engine} & \textbf{Status}\\ \hline
Chrome & Blink & \textbf{Pass} \\ \hline
Brave & Chromium & \textbf{Pass} \\ \hline
Firefox & Gecko & \textbf{Pass} \\ \hline
Safari & WebKit & \textbf{Pass} \\ \hline
\end{tabular}
\caption{Cross-Browser Compatibility Matrix}
\end{table}



\section{Installation and Usage}
\paragraph{Prerequisites}
\begin{itemize}
    \item Docker installed on the host machine.
    \item A modern web browser (Chrome, Firefox, Safari).
\end{itemize}
\paragraph{Installation Steps}
\begin{enumerate}
    \item Clone the repository:\\
    \texttt{git clone https://github.com/Marcop-00/Face-Detector-Web-Application-In-Browser}
    \item Navigate to the project directory: \texttt{cd Face-Detector-Web-Application-In-Browser}
    \item Build the Docker image: \texttt{docker build -t face-app-secure .}
    \item Run the Docker container: \texttt{docker run -d -p 80:80 -p 443:443 --name face-app face-app-secure}
\end{enumerate}

\paragraph{Start the Application:}
Open a web browser and navigate to \texttt{https://localhost}. You may encounter a security warning due to the self-signed certificate; proceed to the site to access the application. 

\subsection{Client Usage}
The interface is very intuitive:
\figure[H]
    \centering
    \includegraphics[width=0.9\textwidth]{img/Screenshot 2026-02-06 at 10.10.43.png}
    \caption{Face Detection Application Interface}
\endfigure

\subsection{Controls}
The application provides a simple control panel located at the bottom of the interface:
\begin{itemize}
    \item \textbf{Start/Stop Camera Button:} Toggles the webcam stream and face detection process.
    \item \textbf{Snapshot Button:} Captures the current video frame and detected faces, allowing users to download the image locally (with camera on).
    \item \textbf{Blur Toggle:} Enables or disables the face blurring effect for enhanced privacy (with camera on).
    \item \textbf{Recording Button:} Starts and stops video recording with face detection overlays, saving the output as a local file (with camera on).
\end{itemize}

\section{Validation Results}
\subsection{Efficiency}
With an optimized prompt the AI generated in a one shot prototype that correctly implemented complex client-side Machine Learning libraries (MediaPipe). The initial development time was reduced significantly.
Using this strategy I was able to get a working prototype in few minutes, while without it I would have probably spent hours just trying to figure out how to load the model and how to structure the code.

\subsection{CSP Security Refinements}
The initial CSP generated by the AI was functional but contained inline comments that caused parsing errors. After refining the prompt to remove comments and include necessary directives for WebAssembly, the final CSP was successfully implemented, ensuring that the application only connects to authorized sources for scripts, styles, fonts, and model assets.
The AI suggested me to use a wasm-unsafe-eval directive giving at least the permission to compile the wasm binaries, but no other connections are allowed.\\
But trying this CSP I was still receiving errors in the console about blocked connections to some external sources.\\
I thought that pursuing this way was not the best solution, because I would have to allow some external connections, and I wanted to have a fully secure configuration, so I decided to ask the AI if downloading all the assets locally was a good solution.\\
The application achieved a fully secure configuration with no external network requests, as verified through browser developer tools.
\subsection{HTTPS Constraints and Media Access}
A critical issue encountered during validation was the browser's strict enforcement of security policies regarding hardware access. Modern browsers (Chrome, Safari, Brave) block access to media devices (`navigator.mediaDevices.getUserMedia`) on non-secure contexts (HTTP), unless the origin is strictly `localhost`.

To resolve the HTTPS constraint while maintaining a portable, "runnable out-of-the-box" experience for evaluation, the Docker container was configured to handle its own encryption.

\textbf{Implementation Detail:}
Unlike a production environment where certificates are managed by a Certificate Authority (CA), this project utilizes \textbf{ephemeral self-signed certificates}. The `Dockerfile` includes an `OpenSSL` command that generates a fresh 2048-bit RSA key and certificate pair dynamically during the image build process.

\textbf{Security Acknowledgment:}
I acknowledge that while this approach secures the connection technically (enabling the `Secure Context`), it triggers browser warnings because the certificate is not signed by a trusted root CA. In a production scenario, this would be replaced with:
\begin{enumerate}
    \item \textbf{Trusted Certificates:} Using an automated solution like Let's Encrypt or a commercially signed certificate.
    \item \textbf{Secret Management:} Ensuring private keys are injected via secure environment variables or secret managers, rather than being generated or stored within the container file system.
\end{enumerate}

The current "generation-on-build" strategy was chosen specifically to ensure the examiner can run the project immediately without needing to manually generate or mount SSL credentials.


\section{Conclusion}

This project successfully demonstrated the efficacy of an AI-assisted workflow in modern web development. By utilizing a "Meta-Prompting" strategy, the initial development time was reduced significantly, yielding a functional "One-Shot" prototype that correctly implemented complex client-side Machine Learning libraries (MediaPipe).\\
The interesting thing is that the AI didn't generated a complete, very secure CSP at first and usable on different environments, but with proper prompting it was able to refine it until it worked perfectly. Another interesting aspect is that the AI didn't lose the context also after multiple prompts, as happens sometimes with other LLMs or less advanced models.\\
The final result is a zero-latency, privacy-first application that is easy to deploy and maintain.
I think that this project can validates the potential of the AI today, but it can be interesting to see how it behave in different scenarios, with different constraints and maybe with more complex requirements, to see if it can still be able to deliver a working solution in one shot or if it needs more iterations and refinements.\\
My final considerations are that the AI still needs a human in the loop for proper prompt engineering and to guide it through complex requirements, but the overall development process was significantly accelerated.
Obviously, the human contribution was still crucial to achieve the final result because without a context or the right competences the AI would have not been able to deliver the expected results or maybe yes, but not interpretable and not maintainable.\\


\clearpage

\begin{thebibliography}{1}

\bibitem{yang2023opro}
Yang, C., Wang, X., Lu, Y., Liu, H., Le, Q. V., \& Zhou, D. (2023). 
\textit{Large Language Models as Optimizers}. 
arXiv preprint arXiv:2309.03409. 
\url{https://arxiv.org/abs/2309.03409}

\end{thebibliography}

\end{document}